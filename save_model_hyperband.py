"""
Script to create a model builder function and hyperparameter search space for Hyperband tuning.

This creates:
1. A Python file with model builder function in /models
2. A JSON file with hyperparameter search space in /models

The model builder function takes a keras_tuner.HyperParameters object and returns a compiled model.
"""
import json
from pathlib import Path


# =============================================================================
# CONFIGURATION
# =============================================================================

MODEL_NAME = "lstm_hyperband_tuner"
NUM_FEATURES = 5        # Number of time series features
TIMESTEPS = 128         # Length of time series
NUM_CLASSES = 3         # Number of output classes

# =============================================================================
# HYPERPARAMETER SEARCH SPACE DEFINITION
# =============================================================================

# Define the hyperparameter search space
# Each hyperparameter has: name, type, and range/values
HYPERPARAMETER_SPACE = {
    "model_config": {
        "num_features": NUM_FEATURES,
        "timesteps": TIMESTEPS,
        "num_classes": NUM_CLASSES
    },
    "hyperparameters": [
        {
            "name": "lstm_units_1",
            "type": "int",
            "min_value": 64,
            "max_value": 256,
            "step": 32,
            "description": "Number of units in first LSTM layer"
        },
        {
            "name": "lstm_units_2",
            "type": "int",
            "min_value": 32,
            "max_value": 128,
            "step": 16,
            "description": "Number of units in second LSTM layer"
        },
        {
            "name": "dropout_rate",
            "type": "float",
            "min_value": 0.1,
            "max_value": 0.5,
            "step": 0.1,
            "description": "Dropout rate for regularization"
        },
        {
            "name": "dense_units",
            "type": "int",
            "min_value": 32,
            "max_value": 128,
            "step": 16,
            "description": "Number of units in dense layer"
        },
        {
            "name": "learning_rate",
            "type": "choice",
            "values": [0.0001, 0.0005, 0.001, 0.005, 0.01],
            "description": "Learning rate for optimizer"
        },
        {
            "name": "use_bidirectional",
            "type": "boolean",
            "description": "Whether to use bidirectional LSTM"
        },
        {
            "name": "num_lstm_layers",
            "type": "int",
            "min_value": 1,
            "max_value": 3,
            "step": 1,
            "description": "Number of LSTM layers"
        }
    ]
}

# =============================================================================
# MODEL BUILDER FUNCTION CODE
# =============================================================================

MODEL_BUILDER_CODE = '''"""
Model builder function for Hyperband hyperparameter tuning.
Auto-generated by save_model_hyperband.py
"""
import keras
from keras import layers


def build_model(hp):
    """
    Build and compile a model with hyperparameters from keras_tuner.

    Args:
        hp: keras_tuner.HyperParameters object

    Returns:
        Compiled Keras model
    """
    # Get model configuration
    num_features = {num_features}
    timesteps = {timesteps}
    num_classes = {num_classes}

    # Sample hyperparameters
    lstm_units_1 = hp.Int('lstm_units_1', min_value=64, max_value=256, step=32)
    lstm_units_2 = hp.Int('lstm_units_2', min_value=32, max_value=128, step=16)
    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)
    dense_units = hp.Int('dense_units', min_value=32, max_value=128, step=16)
    learning_rate = hp.Choice('learning_rate', values=[0.0001, 0.0005, 0.001, 0.005, 0.01])
    use_bidirectional = hp.Boolean('use_bidirectional')
    num_lstm_layers = hp.Int('num_lstm_layers', min_value=1, max_value=3, step=1)

    # Build model
    model = keras.Sequential(name='{model_name}')
    model.add(layers.Input(shape=(timesteps, num_features)))

    # Add LSTM layers
    for i in range(num_lstm_layers):
        return_sequences = (i < num_lstm_layers - 1)  # Last layer doesn't return sequences

        if i == 0:
            units = lstm_units_1
        else:
            units = lstm_units_2

        if use_bidirectional:
            model.add(layers.Bidirectional(layers.LSTM(units, return_sequences=return_sequences)))
        else:
            model.add(layers.LSTM(units, return_sequences=return_sequences))

        model.add(layers.Dropout(dropout_rate))

    # Dense layers
    model.add(layers.Dense(dense_units, activation='relu'))
    model.add(layers.Dropout(dropout_rate))

    # Output layer
    model.add(layers.Dense(num_classes, activation='softmax'))

    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model
'''

# =============================================================================
# SAVE FILES
# =============================================================================

# Ensure models directory exists
models_dir = Path(__file__).parent / "models"
models_dir.mkdir(exist_ok=True)

print("=" * 70)
print("CREATING HYPERBAND TUNING FILES")
print("=" * 70)

# 1. Save hyperparameter search space as JSON
hyperparams_path = models_dir / f"{MODEL_NAME}_hyperparams.json"
with open(hyperparams_path, 'w') as f:
    json.dump(HYPERPARAMETER_SPACE, f, indent=2)

print(f"\n1. Hyperparameter search space saved to:")
print(f"   {hyperparams_path}")
print(f"\n   Configuration:")
print(f"   - Model: {MODEL_NAME}")
print(f"   - Input shape: ({TIMESTEPS}, {NUM_FEATURES})")
print(f"   - Output classes: {NUM_CLASSES}")
print(f"   - Hyperparameters to tune: {len(HYPERPARAMETER_SPACE['hyperparameters'])}")

# 2. Save model builder function as Python file
builder_path = models_dir / f"{MODEL_NAME}_builder.py"
formatted_code = MODEL_BUILDER_CODE.format(
    model_name=MODEL_NAME,
    num_features=NUM_FEATURES,
    timesteps=TIMESTEPS,
    num_classes=NUM_CLASSES
)

with open(builder_path, 'w') as f:
    f.write(formatted_code)

print(f"\n2. Model builder function saved to:")
print(f"   {builder_path}")

# 3. Display hyperparameters
print(f"\n3. Hyperparameters to be tuned:")
print("-" * 70)
for hp in HYPERPARAMETER_SPACE['hyperparameters']:
    print(f"\n   â€¢ {hp['name']}")
    print(f"     Type: {hp['type']}")
    if hp['type'] == 'int':
        print(f"     Range: {hp['min_value']} to {hp['max_value']} (step: {hp['step']})")
    elif hp['type'] == 'float':
        print(f"     Range: {hp['min_value']} to {hp['max_value']} (step: {hp['step']})")
    elif hp['type'] == 'choice':
        print(f"     Values: {hp['values']}")
    elif hp['type'] == 'boolean':
        print(f"     Values: True or False")
    print(f"     Description: {hp['description']}")

print("\n" + "=" * 70)
print("SETUP COMPLETE")
print("=" * 70)
print(f"\nYou can now use these files with experiment_hyperband.py by setting:")
print(f'  MODEL_NAME = "{MODEL_NAME}"')
print("\nThe Hyperband tuner will search over these hyperparameters to find")
print("the optimal configuration for your data.")
print("=" * 70)
